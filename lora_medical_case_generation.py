import os
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm

# ============= é…ç½®éƒ¨åˆ† =============
MODEL_REPO = "meta-llama/Llama-3.1-8B-Instruct"
LORA_ADAPTER = "Easonwangzk/lora-llama31-med-adapter"
DATA_PATH = "Rad_filtered_data_final_v8.csv"  # ä¿®æ”¹ä¸ºä½ çš„æ•°æ®è·¯å¾„
OUTPUT_PATH = "lora_comparison_results.csv"
NUM_SAMPLES = 10  # æµ‹è¯•æ ·æœ¬æ•°é‡
MAX_NEW_TOKENS = 256  # æœ€å¤§ç”Ÿæˆé•¿åº¦
MAX_SEQ_LEN = 512  # æœ€å¤§è¾“å…¥é•¿åº¦

# ============= åˆå§‹åŒ–æ¨¡å‹å’Œ Tokenizer =============
print("æ­£åœ¨åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_REPO, use_fast=True)
tokenizer.padding_side = "left"

# è®¾ç½® pad token
if tokenizer.pad_token is None:
    if tokenizer.eos_token is not None:
        tokenizer.pad_token = tokenizer.eos_token
    else:
        tokenizer.add_special_tokens({"pad_token": "[PAD]"})

# æ£€æµ‹æ˜¯å¦æ”¯æŒ bfloat16
use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8
torch_dtype = torch.bfloat16 if use_bf16 else torch.float16

print(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ {MODEL_REPO}...")
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_REPO,
    torch_dtype=torch_dtype,
    device_map="auto",
)
base_model.config.pad_token_id = tokenizer.pad_token_id
base_model.eval()

print(f"æ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨ {LORA_ADAPTER}...")
lora_model = PeftModel.from_pretrained(base_model, LORA_ADAPTER)
lora_model.eval()

# ============= ç”Ÿæˆå‡½æ•° =============
@torch.no_grad()
def generate_answer(model, prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:
    """
    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ

    Args:
        model: è¯­è¨€æ¨¡å‹
        prompt: è¾“å…¥æç¤ºè¯
        max_new_tokens: æœ€å¤§ç”Ÿæˆçš„ token æ•°é‡

    Returns:
        ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬
    """
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_SEQ_LEN
    )
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç ç¡®ä¿å¯å¤ç°
        pad_token_id=tokenizer.pad_token_id,
    )

    gen = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # æå– "Answer:" åé¢çš„å†…å®¹
    return gen.split("Answer:")[-1].strip()

def create_prompt(impression: str) -> str:
    """
    ä¸ºæ”¾å°„å­¦æ¡ˆä¾‹åˆ›å»ºæç¤ºè¯

    Args:
        impression: å½±åƒå­¦å°è±¡/æŠ¥å‘Š

    Returns:
        æ ¼å¼åŒ–çš„æç¤ºè¯
    """
    messages = [
        {
            "role": "system",
            "content": "You are an expert radiologist. Provide accurate, evidence-based answers using the provided medical context."
        },
        {
            "role": "user",
            "content": f"Context:\n{impression}\n\nQuestion: What are the key findings in this case?\n\nAnswer:"
        }
    ]
    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# ============= åŠ è½½æ•°æ® =============
print(f"\næ­£åœ¨åŠ è½½å‰ {NUM_SAMPLES} ä¸ªæ¡ˆä¾‹...")
df = pd.read_csv(DATA_PATH, nrows=NUM_SAMPLES)
print(f"æˆåŠŸåŠ è½½ {len(df)} ä¸ªæ¡ˆä¾‹\n")

# ============= åŸºç¡€æ¨¡å‹ç”Ÿæˆ =============
print("=" * 60)
print("ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ...")
print("=" * 60)
base_answers = []
for idx, row in tqdm(df.iterrows(), total=len(df), desc="åŸºç¡€æ¨¡å‹"):
    prompt = create_prompt(row['impression'])
    answer = generate_answer(base_model, prompt)
    base_answers.append(answer)

    # å¯é€‰ï¼šæ‰“å°æ¯ä¸ªæ¡ˆä¾‹çš„ç»“æœ
    if idx < 2:  # åªæ‰“å°å‰ä¸¤ä¸ªæ¡ˆä¾‹ä½œä¸ºç¤ºä¾‹
        print(f"\n--- æ¡ˆä¾‹ {idx + 1} ---")
        print(f"å°è±¡: {row['impression'][:100]}...")
        print(f"åŸºç¡€æ¨¡å‹å›ç­”: {answer[:200]}...\n")

# ============= LoRA æ¨¡å‹ç”Ÿæˆ =============
print("\n" + "=" * 60)
print("ä½¿ç”¨ LoRA å¾®è°ƒæ¨¡å‹ç”Ÿæˆç­”æ¡ˆ...")
print("=" * 60)
lora_answers = []
for idx, row in tqdm(df.iterrows(), total=len(df), desc="LoRA æ¨¡å‹"):
    prompt = create_prompt(row['impression'])
    answer = generate_answer(lora_model, prompt)
    lora_answers.append(answer)

    # å¯é€‰ï¼šæ‰“å°æ¯ä¸ªæ¡ˆä¾‹çš„ç»“æœ
    if idx < 2:  # åªæ‰“å°å‰ä¸¤ä¸ªæ¡ˆä¾‹ä½œä¸ºç¤ºä¾‹
        print(f"\n--- æ¡ˆä¾‹ {idx + 1} ---")
        print(f"å°è±¡: {row['impression'][:100]}...")
        print(f"LoRA æ¨¡å‹å›ç­”: {answer[:200]}...\n")

# ============= ä¿å­˜ç»“æœ =============
results_df = df.copy()
results_df['base_model_answer'] = base_answers
results_df['lora_model_answer'] = lora_answers

# ä¿å­˜åˆ° CSV
results_df.to_csv(OUTPUT_PATH, index=False)
print("\n" + "=" * 60)
print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_PATH}")
print("=" * 60)

# ============= ç®€å•å¯¹æ¯”å±•ç¤º =============
print("\nå¯¹æ¯”ç¤ºä¾‹ï¼ˆå‰ 2 ä¸ªæ¡ˆä¾‹ï¼‰ï¼š\n")
for idx in range(min(2, len(results_df))):
    print(f"{'=' * 60}")
    print(f"æ¡ˆä¾‹ {idx + 1}")
    print(f"{'=' * 60}")
    print(f"ğŸ“‹ åŸå§‹å°è±¡:\n{results_df.iloc[idx]['impression']}\n")
    print(f"ğŸ¤– åŸºç¡€æ¨¡å‹å›ç­”:\n{results_df.iloc[idx]['base_model_answer']}\n")
    print(f"ğŸ¯ LoRA æ¨¡å‹å›ç­”:\n{results_df.iloc[idx]['lora_model_answer']}\n")
